from collections import OrderedDict
import torch
import torch.nn as nn

# Set random seed for reproducibility
torch.manual_seed(42)

def get_data(n):
    # Generate some toy data
    x = torch.rand(n, 1)
    y = (2 * x + 0.5)
    return x, y

class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()

        self.linear = nn.Linear(1, 1)
        
    def forward(self, x):
        return self.linear(x)

def perform_gradient_descent(x, y, model, optim, loss_fn, num_iters):
    
    for i in range(num_iters):
        
        # Calculate model predictions
        y_pred = model(x)
        
        # Calculate mean squared error
        loss = loss_fn(y_pred, y)
        
        # Calculate gradients
        loss.backward()
        
        # Update parameters
        optim.step()
        
        # Manually zero gradients
        optim.zero_grad()
        
        # Print progress every 100 iterations
        if (i + 1) % 200 == 0:
            print(f"Iteration {i + 1}: Loss = {loss.item():.4f}")
            print (f" w = {model.linear.weight.item():.3f}, b = {model.linear.bias.item():.3f}")
    
    print(f"Iteration {i + 1}: Loss = {loss.item():.4f}")
    print (f" w = {model.linear.weight.item():.3f}, b = {model.linear.bias.item():.3f}")

if __name__ == "__main__":
    
    x, y = get_data(1000)
    
    model = LinearRegression()
    loss = nn.MSELoss()
    optim = torch.optim.SGD(model.parameters(), lr=0.1)
    
    perform_gradient_descent(x, y, model, optim, loss, 1000)
    
    
