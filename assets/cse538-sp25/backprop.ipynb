{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Chain rule for Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input values and parameters with gradient tracking enabled\n",
    "x0 = torch.tensor(1.0)\n",
    "x1 = torch.tensor(2.0)\n",
    "w0 = torch.tensor(0.5)\n",
    "w1 = torch.tensor(-0.3)\n",
    "w2 = torch.tensor(0.1)  # Bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the linear transformation: z = w0*x0 + w1*x1 + w2\n",
    "z = w0 * x0 + w1 * x1 + w2\n",
    "k = torch.exp(-z)\n",
    "h = 1 + k\n",
    "f = 1/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output f (sigmoid(z)): 0.5\n",
      "Output of h: 2.0\n",
      "Output of k: 1.0\n",
      "Output of z: -2.2351741790771484e-08\n"
     ]
    }
   ],
   "source": [
    "# Print the output and gradients\n",
    "print(\"Output f (sigmoid(z)):\", f.item())\n",
    "print (\"Output of h:\", h.item())\n",
    "print (\"Output of k:\", k.item())\n",
    "print (\"Output of z:\", z.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient df/dh: -0.25\n",
      "Gradient df/dw0: 0.25\n",
      "Gradient df/dk: 1.0\n",
      "Gradient df/dz: -1.0\n",
      "Gradient df/dw1: 0.5\n",
      "Gradient df/dw2: 0.25\n",
      "Gradient df/dx0: 0.125\n",
      "Gradient df/dx1: -0.07500000298023224\n"
     ]
    }
   ],
   "source": [
    "# Gradient of f wrt h (df/dw0) = Gradient of f wrt h * Gradient of h wrt k * Gradient of k wrt z * Gradient of z wrt w0\n",
    "df_dh = -1/(h**2)\n",
    "dh_dk = 1.0\n",
    "dk_dz = -torch.exp(-z)\n",
    "dz_dw0 = x0\n",
    "df_dw0 = df_dh * dh_dk * dk_dz * dz_dw0\n",
    "print (\"Gradient df/dh:\", df_dh.item())\n",
    "print(\"Gradient df/dw0:\", df_dw0.item())\n",
    "print (\"Gradient df/dk:\", dh_dk)\n",
    "print (\"Gradient df/dz:\", dk_dz.item())\n",
    "\n",
    "# Gradient of f wrt w1 (df/dw1) = Gradient of f wrt h * Gradient of h wrt k * Gradient of k wrt z * Gradient of z wrt w1\n",
    "dz_dw1 = x1\n",
    "df_dw1 = df_dh * dh_dk * dk_dz * dz_dw1\n",
    "print(\"Gradient df/dw1:\", df_dw1.item())\n",
    "\n",
    "# Gradient of f wrt w2 (df/dw2) = Gradient of f wrt h * Gradient of h wrt k * Gradient of k wrt z * Gradient of z wrt w2\n",
    "dz_dw2 = 1.0\n",
    "df_dw2 = df_dh * dh_dk * dk_dz * dz_dw2\n",
    "print(\"Gradient df/dw2:\", df_dw2.item())\n",
    "\n",
    "# Gradient of f wrt x0 (df/dx0) = Gradient of f wrt h * Gradient of h wrt k * Gradient of k wrt z * Gradient of z wrt x0\n",
    "df_dx0 = df_dh * dh_dk * dk_dz * w0\n",
    "print(\"Gradient df/dx0:\", df_dx0.item())\n",
    "\n",
    "# Gradient of f wrt x1 (df/dx1) = Gradient of f wrt h * Gradient of h wrt k * Gradient of k wrt z * Gradient of z wrt x1\n",
    "df_dx1 = df_dh * dh_dk * dk_dz * w1\n",
    "print(\"Gradient df/dx1:\", df_dx1.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Back propagation using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input values and parameters with gradient tracking enabled\n",
    "x0 = torch.tensor(1.0, requires_grad=True)\n",
    "x1 = torch.tensor(2.0, requires_grad=True)\n",
    "w0 = torch.tensor(0.5, requires_grad=True)\n",
    "w1 = torch.tensor(-0.3, requires_grad=True)\n",
    "w2 = torch.tensor(0.1, requires_grad=True)  # Bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the linear transformation: z = w0*x0 + w1*x1 + w2\n",
    "z = w0 * x0 + w1 * x1 + w2\n",
    "# Retain the gradient on z for inspection (z is an intermediate tensor)\n",
    "z.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Sigmoid activation function: f = sigmoid(z)\n",
    "k = torch.exp(-z)\n",
    "k.retain_grad()\n",
    "\n",
    "h = 1 + k\n",
    "h.retain_grad()\n",
    "\n",
    "f = 1/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backpropagation to compute gradients\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output f (sigmoid(z)): 0.5\n",
      "Output of h: 2.0\n",
      "Gradient df/dh: -0.25\n",
      "Output of k: 1.0\n",
      "Gradient df/dk: -0.25\n",
      "Output of z: -2.2351741790771484e-08\n",
      "Gradient df/dz (z.grad): 0.25\n",
      "Gradient df/dw0: 0.25\n",
      "Gradient df/dw1: 0.5\n",
      "Gradient df/dw2: 0.25\n",
      "Gradient df/dx0: 0.125\n",
      "Gradient df/dx1: -0.07500000298023224\n"
     ]
    }
   ],
   "source": [
    "# Print the output and gradients\n",
    "print(\"Output f (sigmoid(z)):\", f.item())\n",
    "print (\"Output of h:\", h.item())\n",
    "print (\"Gradient df/dh:\", h.grad.item())\n",
    "print (\"Output of k:\", k.item())\n",
    "print (\"Gradient df/dk:\", k.grad.item())\n",
    "print (\"Output of z:\", z.item())\n",
    "print(\"Gradient df/dz (z.grad):\", z.grad.item())\n",
    "print(\"Gradient df/dw0:\", w0.grad.item())\n",
    "print(\"Gradient df/dw1:\", w1.grad.item())\n",
    "print(\"Gradient df/dw2:\", w2.grad.item())\n",
    "print(\"Gradient df/dx0:\", x0.grad.item())\n",
    "print(\"Gradient df/dx1:\", x1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected df/dz (from sigmoid derivative): 0.25\n"
     ]
    }
   ],
   "source": [
    "# Optionally, verify the derivative of the Sigmoid function manually\n",
    "# The derivative of sigmoid is: sigmoid(z) * (1 - sigmoid(z))\n",
    "expected_dfdz = f.item() * (1 - f.item())\n",
    "print(\"Expected df/dz (from sigmoid derivative):\", expected_dfdz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlatk_py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
